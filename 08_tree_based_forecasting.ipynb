{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting: XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import shapiro\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "full_data = pd.read_csv(\"./data/goog.csv\", usecols=[0, 1, 2, 3, 4, 5], parse_dates=True, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.isnull().sum().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Observations\n",
    "1. The dataset contains 2989 observations.\n",
    "2.  The attributes are all numeric except for a date column that is used as the index.\n",
    "3.  There are no missing values in the dataset.\n",
    "4.  The variable Close can be used as a target for the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If we had categorical columns, we would need to identify them and encode them or extract a feature from it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find High Correlated Columns\n",
    "\n",
    "Correlation measures the strength and direction of a linear relationship between two numerical variables. It ranges from -1 to 1:\n",
    "- +1: perfect positive correlation (they move together)\n",
    "- 0: no linear relationship\n",
    "- -1: perfect negative correlation (they move in opposite directions)\n",
    "\n",
    "Understanding correlation helps identify redundant features in a dataset. Removing highly correlated columns can simplify models, reduce overfitting, and improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get pairwise correlation matrix between numeric columns\n",
    "corr = full_data.corr(numeric_only=True)\n",
    "\n",
    "# get the absolute values, now our only interest is to know if they're highly correlated\n",
    "cor_matrix = corr.abs()\n",
    "upper_triangle_matrix = cor_matrix.where(np.triu(np.ones(cor_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# set a correlation threshold\n",
    "corr_th=0.90\n",
    "\n",
    "# Create a list of columns to frop in case thye have a higher correlation than the thrshold\n",
    "drop_list = [col for col in upper_triangle_matrix.columns if any(upper_triangle_matrix[col] > corr_th)]\n",
    "drop_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When analyzing feature correlation, we often remove highly correlated input variables to reduce redundancy and improve model performance. However, if a **column is your target variable** (the one you‚Äôre trying to predict), it **should never be dropped**, even if it‚Äôs highly correlated with other features. In fact, a strong correlation between features and the target is often a **good sign**, as it means the model can learn meaningful relationships. You should only drop **input features** that are highly correlated with each other ‚Äî not with the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot matrix heatmap\n",
    "sns.set(rc={'figure.figsize': (10, 8)})\n",
    "sns.heatmap(corr, cmap=\"RdBu\", annot=True, fmt=\".2f\", vmin=-1, vmax=1)\n",
    "plt.title(f\"Matriz de correlaci√≥n (umbral = {corr_th})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of keeping multiple highly correlated features like `High` and `Low`, we can take advantage of their relationship by engineering a **new, more informative feature**. \n",
    "\n",
    "For example, the difference between `High` and `Low` reflects the daily price range, which captures volatility in a single value. This reduces redundancy while preserving valuable information. By creating a feature like `price_range = High - Low`, we simplify the model input without losing predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data[\"price_range\"] = full_data[\"High\"] - full_data[\"Low\"]\n",
    "full_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normality of Numerical Values\n",
    "\n",
    "The **Shapiro-Wilk test** is a statistical test used to assess whether a numeric variable is **normally distributed**.\n",
    "- Null hypothesis (H‚ÇÄ): the data **follows** a normal distribution.\n",
    "- Alternative hypothesis (H‚ÇÅ): the data **does not** follow a normal distribution.\n",
    "\n",
    "If the **p-value < 0.05**, we reject the null hypothesis ‚Äî meaning the data is **not normally distributed**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä Shapiro-Wilk Normality Test Resultsüìä\")\n",
    "print(\"==========================================\\n\")\n",
    "\n",
    "num_cols = [col for col in full_data.columns if full_data[col].dtypes in [\"int\", \"float\"]]\n",
    "\n",
    "for col in num_cols:\n",
    "    stat, p = shapiro(full_data[col].dropna())  \n",
    "    if p > 0.05:\n",
    "        result = \"‚úÖ Likely Normally Distributed\"\n",
    "    else:\n",
    "        result = \"‚ùå Not Normally Distributed\"\n",
    "\n",
    "    print(f\"üìå **{col}**\\n   - p-value: {p:.5f} ‚Üí {result}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check this nornality in a visual way, as well, let's plot an what an example of a Normal Distribution would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate normal values\n",
    "data = np.random.normal(loc=0, scale=1, size=1000)\n",
    "\n",
    "# plot\n",
    "sns.histplot(data, kde=True)\n",
    "plt.title(\"Example of a Normal Distribution\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "num_cols = [col for col in full_data.columns if full_data[col].dtypes in [\"int\", \"float\"]]\n",
    "\n",
    "for col in num_cols:\n",
    "    plt.figure()\n",
    "    sns.histplot(full_data[col], kde=True)\n",
    "    plt.title(f\"Histogram of {col}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Summmary of observations\n",
    "\n",
    "### General Observations\n",
    "- The dataset contains **2,989 rows** and **6 numerical columns**, plus one **date column**.\n",
    "- There are **no missing values** in the dataset.\n",
    "- All columns are numeric except for the **`Date`** column, which can be used later for time-based features.\n",
    "- The **`Close`** column is a good candidate for the **target variable** in predictive models.\n",
    "\n",
    "---\n",
    "\n",
    "### Correlation Analysis\n",
    "\n",
    "- We found that **`High`**, **`Low`**, and **`Close`** are **highly correlated** (correlation > 0.90).\n",
    "- Since `Close` will be used as the **target**, it should **not be dropped**, even if it's highly correlated with other features.\n",
    "- To reduce feature redundancy, we engineered a new variable:\n",
    "  ```python\n",
    "  full_data[\"price_range\"] = full_data[\"High\"] - full_data[\"Low\"]\n",
    "\n",
    "---\n",
    "\n",
    "### Normality Testing\n",
    "- We applied the **Shapiro-Wilk test to check** for normal distribution in numeric variables.\n",
    "- All features returned **p-values < 0.0001**, meaning we **reject the null hypothesis** of normality.\n",
    "- This indicates that the variables are **not normally distributed**, which is common in financial datasets due to **skewness** and **outliers**.\n",
    "- However, since we plan to use tree-based models like XGBoost, **normality is not required** and does not impact model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop those columns we decided aren't going to add any value or that the model can't handle. \n",
    "\n",
    "Here we'll separate a 10% of the data for later testing and split de target variable as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(full_data)\n",
    "test_size = int(n * 0.1)\n",
    "\n",
    "train = full_data.iloc[:-test_size]\n",
    "test = full_data.iloc[-test_size:]\n",
    "\n",
    "columns_to_drop = ['High', 'Low']\n",
    "\n",
    "X = train.copy()\n",
    "X.drop(columns_to_drop, axis = 1, inplace = True)\n",
    "y = X.pop('Close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into Train and Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.kdeplot(y_train, label='Train', fill=True)\n",
    "sns.kdeplot(y_val, label='Validation', fill=True)\n",
    "plt.title(\"Distribution of Target Variable in Train vs Validation Sets\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: [TimeSeriesSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we had categorial columns or missing values in pur columns, we'd need to implement some preprocessing steps with sklearn's preprocessing capabilities, something like:\n",
    "\n",
    "```python\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "])\n",
    "\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('cat', categorical_transformer, new_cat_cols),\n",
    "    ('num', numerical_transformer, new_num_cols)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Machine Learning Models and Params to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case we want to test more models, here we can add them in the dicts\n",
    "models = {\n",
    "    'XGBoost': XGBRegressor(random_state=42, verbosity=0)\n",
    "}\n",
    "\n",
    "# we would also need to stablish parameters for those model here\n",
    "param_grid = {\n",
    "    'XGBoost': {\n",
    "    'classifier__n_estimators': [100, 200],\n",
    "    'classifier__learning_rate': [0.01, 0.1],\n",
    "    'classifier__max_depth': [3, 5],\n",
    "    'classifier__min_child_weight': [1, 3],\n",
    "    'classifier__subsample': [0.8],\n",
    "    'classifier__colsample_bytree': [0.8]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization\n",
    "\n",
    "To optimize model performance, we will use [**Grid Search**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html), a technique that systematically explores a predefined set of hyperparameters. This method evaluates all possible combinations using cross-validation to find the configuration that yields the **best results** for the given task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep results\n",
    "best_models = {}\n",
    "model_scores = []\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"üîç Training & Tuning {model_name}...\")\n",
    "\n",
    "    # Find best params\n",
    "    grid_search = RandomizedSearchCV(\n",
    "        estimator=model,\n",
    "        param_distributions=param_grid[model_name],\n",
    "        n_iter=20,\n",
    "        cv=3,\n",
    "        scoring='r2',\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Keep best model\n",
    "    best_models[model_name] = grid_search.best_estimator_\n",
    "\n",
    "    # Predict\n",
    "    y_pred = grid_search.best_estimator_.predict(X_val)\n",
    "\n",
    "    # Evaluate\n",
    "    r2 = r2_score(y_val, y_pred)\n",
    "    mse = mean_squared_error(y_val, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "\n",
    "    # Keep scores\n",
    "    model_scores.append([model_name, r2, mse, rmse, mae])\n",
    "\n",
    "    # Log summary\n",
    "    print(f\"‚úÖ Finished training {model_name}\")\n",
    "    print(f\"   R¬≤: {r2:.4f} | MSE: {mse:.4f} | RMSE: {rmse:.4f} | MAE: {mae:.4f}\")\n",
    "    print(\"------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = pd.DataFrame(model_scores, columns=[\"Model\", \"R¬≤\", \"MSE\", \"RMSE\", \"MAE\"])\n",
    "scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test.copy()\n",
    "y_test = X_test.pop('Close')\n",
    "X_test.drop(columns_to_drop, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set prediction\n",
    "fina_moldel = best_models['XGBoost']\n",
    "y_test_pred = final_model.predict(X_test)\n",
    "\n",
    "# Create df to compare values over time\n",
    "results = pd.DataFrame({\n",
    "    'Actual': y_test,\n",
    "    'Predicted': y_test_pred\n",
    "}, index=y_test.index)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(results.index, results['Actual'], label='Actual', linewidth=2)\n",
    "plt.plot(results.index, results['Predicted'], label='Predicted', linestyle='--')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Close Price')\n",
    "plt.title('üìà Predicted vs Actual Close Price Over Time (Test Set)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the index is ordered\n",
    "train = train.sort_index()\n",
    "test = test.sort_index()\n",
    "\n",
    "# Add 5 day moving average\n",
    "train['SMA_5'] = train['Close'].rolling(window=5).mean()\n",
    "test['SMA_5'] = test['Close'].rolling(window=5).mean()\n",
    "\n",
    "X_test = test.copy()\n",
    "y_test = X_test.pop('Close')\n",
    "X_test.drop(columns_to_drop, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['High', 'Low']\n",
    "\n",
    "X_5ma = train.copy()\n",
    "X_5ma.drop(columns_to_drop, axis = 1, inplace = True)\n",
    "y_5ma = X_5ma.pop('Close')\n",
    "X_train_5ma, X_val_5ma, y_train_5ma, y_val_5ma = train_test_split(X_5ma, y_5ma, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_models(X_train, X_val, y_train, y_val, models, param_grid, n_iter=20, cv=3, scoring='r2'):\n",
    "    \"\"\"\n",
    "    Trains and evaluates multiple models using RandomizedSearchCV.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train, X_val, y_train, y_val : pd.DataFrame / pd.Series\n",
    "        Training and validation data.\n",
    "    models : dict\n",
    "        Dictionary with model names and their instances.\n",
    "    param_grid : dict\n",
    "        Dictionary of hyperparameters for tuning.\n",
    "    n_iter : int\n",
    "        Number of hyperparameter combinations to try.\n",
    "    cv : int\n",
    "        Number of cross-validation folds.\n",
    "    scoring : str\n",
    "        Metric to optimize.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    best_models : dict\n",
    "        Trained models with the best parameters.\n",
    "    model_scores : list\n",
    "        Evaluation metrics for each model.\n",
    "    \"\"\"\n",
    "    best_models = {}\n",
    "    model_scores = []\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"üîç Training & Tuning {model_name}...\")\n",
    "\n",
    "        grid_search = RandomizedSearchCV(\n",
    "            estimator=model,\n",
    "            param_distributions=param_grid[model_name],\n",
    "            n_iter=n_iter,\n",
    "            cv=cv,\n",
    "            scoring=scoring,\n",
    "            n_jobs=-1,\n",
    "            random_state=42\n",
    "        )\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        best_model = grid_search.best_estimator_\n",
    "        best_models[model_name] = best_model\n",
    "\n",
    "        y_pred = best_model.predict(X_val)\n",
    "\n",
    "        r2 = r2_score(y_val, y_pred)\n",
    "        mse = mean_squared_error(y_val, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_val, y_pred)\n",
    "\n",
    "        model_scores.append([model_name, r2, mse, rmse, mae])\n",
    "\n",
    "        print(f\"‚úÖ Finished training {model_name}\")\n",
    "        print(f\"   R¬≤: {r2:.4f} | MSE: {mse:.4f} | RMSE: {rmse:.4f} | MAE: {mae:.4f}\")\n",
    "        print(\"------------------------------------------------------\")\n",
    "\n",
    "    return best_models, model_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models_5ma, model_scores_5ma = train_and_evaluate_models(\n",
    "    X_train_5ma, X_val_5ma, y_train_5ma, y_val_5ma,\n",
    "    models=models,\n",
    "    param_grid=param_grid\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_actual_vs_predicted_over_time(model, X_test, y_test, title_suffix='Test Set'):\n",
    "    \"\"\"\n",
    "    Makes predictions using the given model and plots actual vs predicted values over time.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : Trained model with a predict method\n",
    "    X_test : pd.DataFrame\n",
    "        Input data for prediction\n",
    "    y_test : pd.Series or pd.DataFrame\n",
    "        Actual values, must have a temporal index (dates)\n",
    "    title_suffix : str, optional\n",
    "        Text to include in the plot title (default is 'Test Set')\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    results : pd.DataFrame\n",
    "        DataFrame with 'Actual' and 'Predicted' columns, indexed like y_test.index\n",
    "    \"\"\"\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    results = pd.DataFrame({\n",
    "        'Actual': y_test,\n",
    "        'Predicted': y_test_pred\n",
    "    }, index=y_test.index)\n",
    "\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(results.index, results['Actual'], label='Actual', linewidth=2)\n",
    "    plt.plot(results.index, results['Predicted'], label='Predicted', linestyle='--')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Close Price')\n",
    "    plt.title(f'üìà Predicted vs Actual Close Price Over Time ({title_suffix})')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = best_models_5ma['XGBoost']\n",
    "results_df = plot_actual_vs_predicted_over_time(final_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise\n",
    "\n",
    "There is significant room for improvement in the model results. To enhance your approach, try the following:\n",
    "1. Implement a different train-test split that respects the temporal nature of the data (i.e., avoid random splits that break the time order).\n",
    "2. Propose and engineer at least two new features that could help reduce the model errors and improve prediction accuracy.\n",
    "  > üí° **Hint:** Study how you can incorporate past context with a feature. Are there more financial indicators studied that we can use?\n",
    "\n",
    "Your goal is to experiment with these changes and observe how they affect the model‚Äôs performance and predictions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
